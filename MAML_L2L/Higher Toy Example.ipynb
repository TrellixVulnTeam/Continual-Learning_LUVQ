{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://higher.readthedocs.io/en/latest/toplevel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import higher\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(3)\n",
    "N = 50\n",
    "actual_multiplier = 5.1\n",
    "meta_lr = 0.00001\n",
    "adaptation_steps = 50 # how many iterations in the inner loop we want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.random.random((N,1)), dtype=torch.float64) # features for inner training loop\n",
    "# print(x)\n",
    "y = x * actual_multiplier # target for inner training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = nn.Linear(1, 1, bias=False).double() # simplest possible model - multiple \n",
    "# input x by weight w without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_opt = optim.SGD(model.parameters(), lr=meta_lr, momentum=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inner_loop_once(model, verbose, copy_initial_weights):\n",
    "    \n",
    "    lr_tensor = torch.tensor([0.3], requires_grad=True)\n",
    "    momentum_tensor = torch.tensor([0.5], requires_grad=True)\n",
    "    opt = optim.SGD(model.parameters(), lr=0.3, momentum=0.5)\n",
    "    with higher.innerloop_ctx(model, opt, copy_initial_weights=copy_initial_weights, override={'lr': lr_tensor, 'momentum': momentum_tensor}) as (fmodel, diffopt):\n",
    "        for j in range(adaptation_steps):\n",
    "            if verbose:\n",
    "                print('Starting inner loop step j=={0}'.format(j))\n",
    "                print('    Representation of fmodel.parameters(time={0}): {1}'.format(j, str(list(fmodel.parameters(time=j)))))\n",
    "                print('    Notice that fmodel.parameters() is same as fmodel.parameters(time={0}): {1}'.format(j, (list(fmodel.parameters())[0] is list(fmodel.parameters(time=j))[0])))\n",
    "            out = fmodel(x)\n",
    "            if verbose:\n",
    "                print('    Notice how `out` is `x` multiplied by the latest version of weight: {0:.4} * {1:.4} == {2:.4}'.format(x[0,0].item(), list(fmodel.parameters())[0].item(), out[0].item()))\n",
    "            loss = ((out - y)**2).mean()\n",
    "            diffopt.step(loss)\n",
    "\n",
    "        if verbose:\n",
    "            # after all inner training let's see all steps' parameter tensors\n",
    "            print()\n",
    "            print(\"Let's print all intermediate parameters versions after inner loop is done:\")\n",
    "            for j in range(adaptation_steps+1):\n",
    "                print('    For j=={0} parameter is: {1}'.format(j, str(list(fmodel.parameters(time=j)))))\n",
    "            print()\n",
    "\n",
    "        # let's imagine now that our meta-learning optimization is trying to check how far we got in the end from the actual_multiplier\n",
    "        weight_learned_after_full_inner_loop = list(fmodel.parameters())[0]\n",
    "        meta_loss = (weight_learned_after_full_inner_loop - actual_multiplier)**2\n",
    "        print('  Final meta-loss: {0}'.format(meta_loss.item()))\n",
    "        meta_loss.backward() # will only propagate gradient to original model parameter's `grad` if copy_initial_weight=False\n",
    "        if verbose:\n",
    "            print('  Gradient of final loss we got for lr and momentum: {0} and {1}'.format(lr_tensor.grad, momentum_tensor.grad))\n",
    "            print('  If you change number of iterations \"loops\" to much larger number final loss will be stable and the values above will be smaller')\n",
    "        return meta_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "=================== Run Inner Loop First Time (copy_initial_weights=True) =================\n",
      "\n",
      "Starting inner loop step j==0\n",
      "    Representation of fmodel.parameters(time=0): [tensor([[-0.9915]], dtype=torch.float64, requires_grad=True)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=0): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * -0.9915 == -0.4135\n",
      "Starting inner loop step j==1\n",
      "    Representation of fmodel.parameters(time=1): [tensor([[0.1462]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=1): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 0.1462 == 0.06095\n",
      "Starting inner loop step j==2\n",
      "    Representation of fmodel.parameters(time=2): [tensor([[1.6401]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=2): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 1.64 == 0.684\n",
      "Starting inner loop step j==3\n",
      "    Representation of fmodel.parameters(time=3): [tensor([[3.0333]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=3): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 3.033 == 1.265\n",
      "Starting inner loop step j==4\n",
      "    Representation of fmodel.parameters(time=4): [tensor([[4.1158]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=4): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 4.116 == 1.716\n",
      "Starting inner loop step j==5\n",
      "    Representation of fmodel.parameters(time=5): [tensor([[4.8409]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=5): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 4.841 == 2.019\n",
      "Starting inner loop step j==6\n",
      "    Representation of fmodel.parameters(time=6): [tensor([[5.2518]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=6): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.252 == 2.19\n",
      "Starting inner loop step j==7\n",
      "    Representation of fmodel.parameters(time=7): [tensor([[5.4289]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=7): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.429 == 2.264\n",
      "Starting inner loop step j==8\n",
      "    Representation of fmodel.parameters(time=8): [tensor([[5.4561]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=8): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.456 == 2.275\n",
      "Starting inner loop step j==9\n",
      "    Representation of fmodel.parameters(time=9): [tensor([[5.4031]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=9): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.403 == 2.253\n",
      "Starting inner loop step j==10\n",
      "    Representation of fmodel.parameters(time=10): [tensor([[5.3200]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=10): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.32 == 2.219\n",
      "Starting inner loop step j==11\n",
      "    Representation of fmodel.parameters(time=11): [tensor([[5.2374]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=11): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.237 == 2.184\n",
      "Starting inner loop step j==12\n",
      "    Representation of fmodel.parameters(time=12): [tensor([[5.1704]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=12): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.17 == 2.156\n",
      "Starting inner loop step j==13\n",
      "    Representation of fmodel.parameters(time=13): [tensor([[5.1238]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=13): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.124 == 2.137\n",
      "Starting inner loop step j==14\n",
      "    Representation of fmodel.parameters(time=14): [tensor([[5.0960]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=14): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.096 == 2.125\n",
      "Starting inner loop step j==15\n",
      "    Representation of fmodel.parameters(time=15): [tensor([[5.0829]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=15): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.083 == 2.12\n",
      "Starting inner loop step j==16\n",
      "    Representation of fmodel.parameters(time=16): [tensor([[5.0795]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=16): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.08 == 2.118\n",
      "Starting inner loop step j==17\n",
      "    Representation of fmodel.parameters(time=17): [tensor([[5.0817]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=17): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.082 == 2.119\n",
      "Starting inner loop step j==18\n",
      "    Representation of fmodel.parameters(time=18): [tensor([[5.0861]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=18): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.086 == 2.121\n",
      "Starting inner loop step j==19\n",
      "    Representation of fmodel.parameters(time=19): [tensor([[5.0910]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=19): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.091 == 2.123\n",
      "Starting inner loop step j==20\n",
      "    Representation of fmodel.parameters(time=20): [tensor([[5.0951]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=20): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.095 == 2.125\n",
      "Starting inner loop step j==21\n",
      "    Representation of fmodel.parameters(time=21): [tensor([[5.0981]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=21): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.098 == 2.126\n",
      "Starting inner loop step j==22\n",
      "    Representation of fmodel.parameters(time=22): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=22): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==23\n",
      "    Representation of fmodel.parameters(time=23): [tensor([[5.1008]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=23): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==24\n",
      "    Representation of fmodel.parameters(time=24): [tensor([[5.1012]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=24): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==25\n",
      "    Representation of fmodel.parameters(time=25): [tensor([[5.1011]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=25): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==26\n",
      "    Representation of fmodel.parameters(time=26): [tensor([[5.1009]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=26): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==27\n",
      "    Representation of fmodel.parameters(time=27): [tensor([[5.1006]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=27): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==28\n",
      "    Representation of fmodel.parameters(time=28): [tensor([[5.1003]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=28): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==29\n",
      "    Representation of fmodel.parameters(time=29): [tensor([[5.1001]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=29): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==30\n",
      "    Representation of fmodel.parameters(time=30): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=30): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==31\n",
      "    Representation of fmodel.parameters(time=31): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=31): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==32\n",
      "    Representation of fmodel.parameters(time=32): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=32): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==33\n",
      "    Representation of fmodel.parameters(time=33): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=33): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==34\n",
      "    Representation of fmodel.parameters(time=34): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=34): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==35\n",
      "    Representation of fmodel.parameters(time=35): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=35): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==36\n",
      "    Representation of fmodel.parameters(time=36): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=36): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==37\n",
      "    Representation of fmodel.parameters(time=37): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=37): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==38\n",
      "    Representation of fmodel.parameters(time=38): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=38): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==39\n",
      "    Representation of fmodel.parameters(time=39): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=39): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==40\n",
      "    Representation of fmodel.parameters(time=40): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=40): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==41\n",
      "    Representation of fmodel.parameters(time=41): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=41): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==42\n",
      "    Representation of fmodel.parameters(time=42): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=42): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==43\n",
      "    Representation of fmodel.parameters(time=43): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=43): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==44\n",
      "    Representation of fmodel.parameters(time=44): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=44): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==45\n",
      "    Representation of fmodel.parameters(time=45): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=45): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==46\n",
      "    Representation of fmodel.parameters(time=46): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=46): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==47\n",
      "    Representation of fmodel.parameters(time=47): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=47): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==48\n",
      "    Representation of fmodel.parameters(time=48): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=48): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==49\n",
      "    Representation of fmodel.parameters(time=49): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=49): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "\n",
      "Let's print all intermediate parameters versions after inner loop is done:\n",
      "    For j==0 parameter is: [tensor([[-0.9915]], dtype=torch.float64, requires_grad=True)]\n",
      "    For j==1 parameter is: [tensor([[0.1462]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==2 parameter is: [tensor([[1.6401]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==3 parameter is: [tensor([[3.0333]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==4 parameter is: [tensor([[4.1158]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==5 parameter is: [tensor([[4.8409]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==6 parameter is: [tensor([[5.2518]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==7 parameter is: [tensor([[5.4289]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==8 parameter is: [tensor([[5.4561]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==9 parameter is: [tensor([[5.4031]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==10 parameter is: [tensor([[5.3200]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==11 parameter is: [tensor([[5.2374]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==12 parameter is: [tensor([[5.1704]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==13 parameter is: [tensor([[5.1238]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==14 parameter is: [tensor([[5.0960]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==15 parameter is: [tensor([[5.0829]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==16 parameter is: [tensor([[5.0795]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==17 parameter is: [tensor([[5.0817]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==18 parameter is: [tensor([[5.0861]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==19 parameter is: [tensor([[5.0910]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==20 parameter is: [tensor([[5.0951]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==21 parameter is: [tensor([[5.0981]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==22 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==23 parameter is: [tensor([[5.1008]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==24 parameter is: [tensor([[5.1012]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==25 parameter is: [tensor([[5.1011]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==26 parameter is: [tensor([[5.1009]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==27 parameter is: [tensor([[5.1006]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==28 parameter is: [tensor([[5.1003]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==29 parameter is: [tensor([[5.1001]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==30 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==31 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==32 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==33 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==34 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==35 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==36 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==37 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==38 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==39 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==40 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==41 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==42 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==43 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==44 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==45 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==46 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==47 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==48 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==49 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==50 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "\n",
      "  Final meta-loss: 3.856774004109851e-14\n",
      "  Gradient of final loss we got for lr and momentum: tensor([1.7909e-12]) and tensor([4.7364e-12])\n",
      "  If you change number of iterations \"loops\" to much larger number final loss will be stable and the values above will be smaller\n",
      "\n",
      "Let's see if we got any gradient for initial model parameters: None\n",
      "\n",
      "####################################################################################################\n",
      "=================== Run Inner Loop Second Time (copy_initial_weights=False) =================\n",
      "\n",
      "Starting inner loop step j==0\n",
      "    Representation of fmodel.parameters(time=0): [tensor([[-0.9915]], dtype=torch.float64, grad_fn=<CloneBackward>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=0): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * -0.9915 == -0.4135\n",
      "Starting inner loop step j==1\n",
      "    Representation of fmodel.parameters(time=1): [tensor([[0.1462]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=1): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 0.1462 == 0.06095\n",
      "Starting inner loop step j==2\n",
      "    Representation of fmodel.parameters(time=2): [tensor([[1.6401]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=2): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 1.64 == 0.684\n",
      "Starting inner loop step j==3\n",
      "    Representation of fmodel.parameters(time=3): [tensor([[3.0333]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=3): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 3.033 == 1.265\n",
      "Starting inner loop step j==4\n",
      "    Representation of fmodel.parameters(time=4): [tensor([[4.1158]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=4): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 4.116 == 1.716\n",
      "Starting inner loop step j==5\n",
      "    Representation of fmodel.parameters(time=5): [tensor([[4.8409]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=5): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 4.841 == 2.019\n",
      "Starting inner loop step j==6\n",
      "    Representation of fmodel.parameters(time=6): [tensor([[5.2518]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=6): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.252 == 2.19\n",
      "Starting inner loop step j==7\n",
      "    Representation of fmodel.parameters(time=7): [tensor([[5.4289]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=7): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.429 == 2.264\n",
      "Starting inner loop step j==8\n",
      "    Representation of fmodel.parameters(time=8): [tensor([[5.4561]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=8): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.456 == 2.275\n",
      "Starting inner loop step j==9\n",
      "    Representation of fmodel.parameters(time=9): [tensor([[5.4031]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=9): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.403 == 2.253\n",
      "Starting inner loop step j==10\n",
      "    Representation of fmodel.parameters(time=10): [tensor([[5.3200]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=10): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.32 == 2.219\n",
      "Starting inner loop step j==11\n",
      "    Representation of fmodel.parameters(time=11): [tensor([[5.2374]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=11): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.237 == 2.184\n",
      "Starting inner loop step j==12\n",
      "    Representation of fmodel.parameters(time=12): [tensor([[5.1704]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=12): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.17 == 2.156\n",
      "Starting inner loop step j==13\n",
      "    Representation of fmodel.parameters(time=13): [tensor([[5.1238]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=13): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.124 == 2.137\n",
      "Starting inner loop step j==14\n",
      "    Representation of fmodel.parameters(time=14): [tensor([[5.0960]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=14): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.096 == 2.125\n",
      "Starting inner loop step j==15\n",
      "    Representation of fmodel.parameters(time=15): [tensor([[5.0829]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=15): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.083 == 2.12\n",
      "Starting inner loop step j==16\n",
      "    Representation of fmodel.parameters(time=16): [tensor([[5.0795]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=16): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.08 == 2.118\n",
      "Starting inner loop step j==17\n",
      "    Representation of fmodel.parameters(time=17): [tensor([[5.0817]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=17): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.082 == 2.119\n",
      "Starting inner loop step j==18\n",
      "    Representation of fmodel.parameters(time=18): [tensor([[5.0861]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=18): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.086 == 2.121\n",
      "Starting inner loop step j==19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Representation of fmodel.parameters(time=19): [tensor([[5.0910]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=19): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.091 == 2.123\n",
      "Starting inner loop step j==20\n",
      "    Representation of fmodel.parameters(time=20): [tensor([[5.0951]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=20): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.095 == 2.125\n",
      "Starting inner loop step j==21\n",
      "    Representation of fmodel.parameters(time=21): [tensor([[5.0981]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=21): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.098 == 2.126\n",
      "Starting inner loop step j==22\n",
      "    Representation of fmodel.parameters(time=22): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=22): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==23\n",
      "    Representation of fmodel.parameters(time=23): [tensor([[5.1008]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=23): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==24\n",
      "    Representation of fmodel.parameters(time=24): [tensor([[5.1012]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=24): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==25\n",
      "    Representation of fmodel.parameters(time=25): [tensor([[5.1011]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=25): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==26\n",
      "    Representation of fmodel.parameters(time=26): [tensor([[5.1009]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=26): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==27\n",
      "    Representation of fmodel.parameters(time=27): [tensor([[5.1006]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=27): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==28\n",
      "    Representation of fmodel.parameters(time=28): [tensor([[5.1003]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=28): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==29\n",
      "    Representation of fmodel.parameters(time=29): [tensor([[5.1001]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=29): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==30\n",
      "    Representation of fmodel.parameters(time=30): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=30): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==31\n",
      "    Representation of fmodel.parameters(time=31): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=31): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==32\n",
      "    Representation of fmodel.parameters(time=32): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=32): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==33\n",
      "    Representation of fmodel.parameters(time=33): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=33): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==34\n",
      "    Representation of fmodel.parameters(time=34): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=34): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==35\n",
      "    Representation of fmodel.parameters(time=35): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=35): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==36\n",
      "    Representation of fmodel.parameters(time=36): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=36): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==37\n",
      "    Representation of fmodel.parameters(time=37): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=37): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==38\n",
      "    Representation of fmodel.parameters(time=38): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=38): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==39\n",
      "    Representation of fmodel.parameters(time=39): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=39): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==40\n",
      "    Representation of fmodel.parameters(time=40): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=40): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==41\n",
      "    Representation of fmodel.parameters(time=41): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=41): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==42\n",
      "    Representation of fmodel.parameters(time=42): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=42): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==43\n",
      "    Representation of fmodel.parameters(time=43): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=43): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==44\n",
      "    Representation of fmodel.parameters(time=44): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=44): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==45\n",
      "    Representation of fmodel.parameters(time=45): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=45): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==46\n",
      "    Representation of fmodel.parameters(time=46): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=46): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==47\n",
      "    Representation of fmodel.parameters(time=47): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=47): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==48\n",
      "    Representation of fmodel.parameters(time=48): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=48): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==49\n",
      "    Representation of fmodel.parameters(time=49): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=49): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "\n",
      "Let's print all intermediate parameters versions after inner loop is done:\n",
      "    For j==0 parameter is: [tensor([[-0.9915]], dtype=torch.float64, grad_fn=<CloneBackward>)]\n",
      "    For j==1 parameter is: [tensor([[0.1462]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==2 parameter is: [tensor([[1.6401]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==3 parameter is: [tensor([[3.0333]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==4 parameter is: [tensor([[4.1158]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==5 parameter is: [tensor([[4.8409]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==6 parameter is: [tensor([[5.2518]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==7 parameter is: [tensor([[5.4289]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==8 parameter is: [tensor([[5.4561]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==9 parameter is: [tensor([[5.4031]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==10 parameter is: [tensor([[5.3200]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==11 parameter is: [tensor([[5.2374]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==12 parameter is: [tensor([[5.1704]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==13 parameter is: [tensor([[5.1238]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==14 parameter is: [tensor([[5.0960]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==15 parameter is: [tensor([[5.0829]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==16 parameter is: [tensor([[5.0795]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==17 parameter is: [tensor([[5.0817]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==18 parameter is: [tensor([[5.0861]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==19 parameter is: [tensor([[5.0910]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==20 parameter is: [tensor([[5.0951]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==21 parameter is: [tensor([[5.0981]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==22 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==23 parameter is: [tensor([[5.1008]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==24 parameter is: [tensor([[5.1012]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==25 parameter is: [tensor([[5.1011]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==26 parameter is: [tensor([[5.1009]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==27 parameter is: [tensor([[5.1006]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==28 parameter is: [tensor([[5.1003]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==29 parameter is: [tensor([[5.1001]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==30 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==31 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==32 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==33 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==34 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==35 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==36 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==37 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==38 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==39 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==40 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==41 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==42 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==43 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==44 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==45 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==46 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==47 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==48 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==49 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==50 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "\n",
      "  Final meta-loss: 3.856774004109851e-14\n",
      "  Gradient of final loss we got for lr and momentum: tensor([1.7909e-12]) and tensor([4.7364e-12])\n",
      "  If you change number of iterations \"loops\" to much larger number final loss will be stable and the values above will be smaller\n",
      "\n",
      "Let's see if we got any gradient for initial model parameters: tensor([[-1.2663e-14]], dtype=torch.float64)\n",
      "\n",
      "****************************************************************************************************\n",
      "=================== Run Inner Loop Third Time (copy_initial_weights=False) =================\n",
      "\n",
      "  Final meta-loss: 3.856774004109851e-14\n",
      "\n",
      "Side-by-side meta_loss_gradient_approximation and gradient computed by `higher` lib: 0.0 VS -1.266e-14\n"
     ]
    }
   ],
   "source": [
    "print(\"*\" * 100)\n",
    "print('=================== Run Inner Loop First Time (copy_initial_weights=True) =================\\n')\n",
    "meta_loss_val1 = run_inner_loop_once(model, verbose=True, copy_initial_weights=True)\n",
    "print(\"\\nLet's see if we got any gradient for initial model parameters: {0}\\n\".format(\n",
    "    list(model.parameters())[0].grad))\n",
    "print(\"#\" * 100)\n",
    "print('=================== Run Inner Loop Second Time (copy_initial_weights=False) =================\\n')\n",
    "meta_loss_val2 = run_inner_loop_once(model, verbose=True, copy_initial_weights=False)\n",
    "print(\"\\nLet's see if we got any gradient for initial model parameters: {0}\\n\".format(\n",
    "    list(model.parameters())[0].grad))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print('=================== Run Inner Loop Third Time (copy_initial_weights=False) =================\\n')\n",
    "final_meta_gradient = list(model.parameters())[0].grad.item()\n",
    "# Now let's double-check `higher` library is actually doing what it promised to do, not just giving us\n",
    "# a bunch of hand-wavy statements and difficult to read code.\n",
    "# We will do a simple SGD step using meta_opt changing initial weight for the training and see how meta loss changed\n",
    "meta_opt.step()\n",
    "meta_opt.zero_grad()\n",
    "meta_step = - meta_lr * final_meta_gradient # how much meta_opt actually shifted inital weight value\n",
    "meta_loss_val3 = run_inner_loop_once(model, verbose=False, copy_initial_weights=False)\n",
    "\n",
    "meta_loss_gradient_approximation = (meta_loss_val3 - meta_loss_val2) / meta_step\n",
    "\n",
    "print()\n",
    "print('Side-by-side meta_loss_gradient_approximation and gradient computed by `higher` lib: {0:.4} VS {1:.4}'.format(\n",
    "    meta_loss_gradient_approximation, final_meta_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "=================== Run Inner Loop Second Time (copy_initial_weights=False) =================\n",
      "\n",
      "Starting inner loop step j==0\n",
      "    Representation of fmodel.parameters(time=0): [tensor([[-0.9915]], dtype=torch.float64, grad_fn=<CloneBackward>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=0): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * -0.9915 == -0.4135\n",
      "Starting inner loop step j==1\n",
      "    Representation of fmodel.parameters(time=1): [tensor([[0.1462]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=1): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 0.1462 == 0.06095\n",
      "Starting inner loop step j==2\n",
      "    Representation of fmodel.parameters(time=2): [tensor([[1.6401]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=2): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 1.64 == 0.684\n",
      "Starting inner loop step j==3\n",
      "    Representation of fmodel.parameters(time=3): [tensor([[3.0333]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=3): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 3.033 == 1.265\n",
      "Starting inner loop step j==4\n",
      "    Representation of fmodel.parameters(time=4): [tensor([[4.1158]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=4): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 4.116 == 1.716\n",
      "Starting inner loop step j==5\n",
      "    Representation of fmodel.parameters(time=5): [tensor([[4.8409]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=5): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 4.841 == 2.019\n",
      "Starting inner loop step j==6\n",
      "    Representation of fmodel.parameters(time=6): [tensor([[5.2518]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=6): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.252 == 2.19\n",
      "Starting inner loop step j==7\n",
      "    Representation of fmodel.parameters(time=7): [tensor([[5.4289]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=7): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.429 == 2.264\n",
      "Starting inner loop step j==8\n",
      "    Representation of fmodel.parameters(time=8): [tensor([[5.4561]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=8): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.456 == 2.275\n",
      "Starting inner loop step j==9\n",
      "    Representation of fmodel.parameters(time=9): [tensor([[5.4031]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=9): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.403 == 2.253\n",
      "Starting inner loop step j==10\n",
      "    Representation of fmodel.parameters(time=10): [tensor([[5.3200]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=10): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.32 == 2.219\n",
      "Starting inner loop step j==11\n",
      "    Representation of fmodel.parameters(time=11): [tensor([[5.2374]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=11): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.237 == 2.184\n",
      "Starting inner loop step j==12\n",
      "    Representation of fmodel.parameters(time=12): [tensor([[5.1704]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=12): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.17 == 2.156\n",
      "Starting inner loop step j==13\n",
      "    Representation of fmodel.parameters(time=13): [tensor([[5.1238]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=13): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.124 == 2.137\n",
      "Starting inner loop step j==14\n",
      "    Representation of fmodel.parameters(time=14): [tensor([[5.0960]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=14): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.096 == 2.125\n",
      "Starting inner loop step j==15\n",
      "    Representation of fmodel.parameters(time=15): [tensor([[5.0829]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=15): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.083 == 2.12\n",
      "Starting inner loop step j==16\n",
      "    Representation of fmodel.parameters(time=16): [tensor([[5.0795]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=16): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.08 == 2.118\n",
      "Starting inner loop step j==17\n",
      "    Representation of fmodel.parameters(time=17): [tensor([[5.0817]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=17): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.082 == 2.119\n",
      "Starting inner loop step j==18\n",
      "    Representation of fmodel.parameters(time=18): [tensor([[5.0861]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=18): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.086 == 2.121\n",
      "Starting inner loop step j==19\n",
      "    Representation of fmodel.parameters(time=19): [tensor([[5.0910]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=19): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.091 == 2.123\n",
      "Starting inner loop step j==20\n",
      "    Representation of fmodel.parameters(time=20): [tensor([[5.0951]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=20): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.095 == 2.125\n",
      "Starting inner loop step j==21\n",
      "    Representation of fmodel.parameters(time=21): [tensor([[5.0981]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=21): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.098 == 2.126\n",
      "Starting inner loop step j==22\n",
      "    Representation of fmodel.parameters(time=22): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=22): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==23\n",
      "    Representation of fmodel.parameters(time=23): [tensor([[5.1008]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=23): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==24\n",
      "    Representation of fmodel.parameters(time=24): [tensor([[5.1012]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=24): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==25\n",
      "    Representation of fmodel.parameters(time=25): [tensor([[5.1011]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=25): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==26\n",
      "    Representation of fmodel.parameters(time=26): [tensor([[5.1009]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=26): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==27\n",
      "    Representation of fmodel.parameters(time=27): [tensor([[5.1006]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=27): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.101 == 2.127\n",
      "Starting inner loop step j==28\n",
      "    Representation of fmodel.parameters(time=28): [tensor([[5.1003]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=28): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==29\n",
      "    Representation of fmodel.parameters(time=29): [tensor([[5.1001]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=29): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==30\n",
      "    Representation of fmodel.parameters(time=30): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=30): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==31\n",
      "    Representation of fmodel.parameters(time=31): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=31): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==32\n",
      "    Representation of fmodel.parameters(time=32): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=32): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==33\n",
      "    Representation of fmodel.parameters(time=33): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=33): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==34\n",
      "    Representation of fmodel.parameters(time=34): [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=34): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==35\n",
      "    Representation of fmodel.parameters(time=35): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=35): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==36\n",
      "    Representation of fmodel.parameters(time=36): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=36): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==37\n",
      "    Representation of fmodel.parameters(time=37): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=37): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==38\n",
      "    Representation of fmodel.parameters(time=38): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=38): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==39\n",
      "    Representation of fmodel.parameters(time=39): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=39): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==40\n",
      "    Representation of fmodel.parameters(time=40): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=40): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==41\n",
      "    Representation of fmodel.parameters(time=41): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=41): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==42\n",
      "    Representation of fmodel.parameters(time=42): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=42): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==43\n",
      "    Representation of fmodel.parameters(time=43): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=43): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==44\n",
      "    Representation of fmodel.parameters(time=44): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=44): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==45\n",
      "    Representation of fmodel.parameters(time=45): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=45): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==46\n",
      "    Representation of fmodel.parameters(time=46): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=46): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==47\n",
      "    Representation of fmodel.parameters(time=47): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=47): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==48\n",
      "    Representation of fmodel.parameters(time=48): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=48): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "Starting inner loop step j==49\n",
      "    Representation of fmodel.parameters(time=49): [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    Notice that fmodel.parameters() is same as fmodel.parameters(time=49): True\n",
      "    Notice how `out` is `x` multiplied by the latest version of weight: 0.417 * 5.1 == 2.127\n",
      "\n",
      "Let's print all intermediate parameters versions after inner loop is done:\n",
      "    For j==0 parameter is: [tensor([[-0.9915]], dtype=torch.float64, grad_fn=<CloneBackward>)]\n",
      "    For j==1 parameter is: [tensor([[0.1462]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==2 parameter is: [tensor([[1.6401]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==3 parameter is: [tensor([[3.0333]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==4 parameter is: [tensor([[4.1158]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==5 parameter is: [tensor([[4.8409]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==6 parameter is: [tensor([[5.2518]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==7 parameter is: [tensor([[5.4289]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==8 parameter is: [tensor([[5.4561]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==9 parameter is: [tensor([[5.4031]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==10 parameter is: [tensor([[5.3200]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==11 parameter is: [tensor([[5.2374]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==12 parameter is: [tensor([[5.1704]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==13 parameter is: [tensor([[5.1238]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==14 parameter is: [tensor([[5.0960]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==15 parameter is: [tensor([[5.0829]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==16 parameter is: [tensor([[5.0795]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==17 parameter is: [tensor([[5.0817]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==18 parameter is: [tensor([[5.0861]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==19 parameter is: [tensor([[5.0910]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==20 parameter is: [tensor([[5.0951]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==21 parameter is: [tensor([[5.0981]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==22 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==23 parameter is: [tensor([[5.1008]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==24 parameter is: [tensor([[5.1012]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==25 parameter is: [tensor([[5.1011]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==26 parameter is: [tensor([[5.1009]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==27 parameter is: [tensor([[5.1006]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==28 parameter is: [tensor([[5.1003]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==29 parameter is: [tensor([[5.1001]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==30 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==31 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==32 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==33 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==34 parameter is: [tensor([[5.0999]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==35 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==36 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==37 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==38 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==39 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==40 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==41 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==42 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==43 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==44 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==45 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==46 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==47 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==48 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==49 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "    For j==50 parameter is: [tensor([[5.1000]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "\n",
      "  Final meta-loss: 3.856774004109851e-14\n",
      "  Gradient of final loss we got for lr and momentum: tensor([1.7909e-12]) and tensor([4.7364e-12])\n",
      "  If you change number of iterations \"loops\" to much larger number final loss will be stable and the values above will be smaller\n",
      "\n",
      "Let's see if we got any gradient for initial model parameters: tensor([[-2.5326e-14]], dtype=torch.float64)\n",
      "\n",
      "****************************************************************************************************\n",
      "=================== Run Inner Loop Third Time (copy_initial_weights=False) =================\n",
      "\n",
      "  Final meta-loss: 3.856774004109851e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Side-by-side meta_loss_gradient_approximation and gradient computed by `higher` lib: 0.0 VS -2.533e-14\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 100)\n",
    "print('=================== Run Inner Loop Second Time (copy_initial_weights=False) =================\\n')\n",
    "meta_loss_val2 = run_inner_loop_once(model, verbose=True, copy_initial_weights=False)\n",
    "print(\"\\nLet's see if we got any gradient for initial model parameters: {0}\\n\".format(\n",
    "    list(model.parameters())[0].grad))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print('=================== Run Inner Loop Third Time (copy_initial_weights=False) =================\\n')\n",
    "final_meta_gradient = list(model.parameters())[0].grad.item()\n",
    "# Now let's double-check `higher` library is actually doing what it promised to do, not just giving us\n",
    "# a bunch of hand-wavy statements and difficult to read code.\n",
    "# We will do a simple SGD step using meta_opt changing initial weight for the training and see how meta loss changed\n",
    "meta_opt.step()\n",
    "meta_opt.zero_grad()\n",
    "meta_step = - meta_lr * final_meta_gradient # how much meta_opt actually shifted inital weight value\n",
    "meta_loss_val3 = run_inner_loop_once(model, verbose=False, copy_initial_weights=False)\n",
    "\n",
    "meta_loss_gradient_approximation = (meta_loss_val3 - meta_loss_val2) / meta_step\n",
    "\n",
    "print()\n",
    "print('Side-by-side meta_loss_gradient_approximation and gradient computed by `higher` lib: {0:.4} VS {1:.4}'.format(\n",
    "    meta_loss_gradient_approximation, final_meta_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4c35c399137e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-76b981bee6a0>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "print(accuracy(torch.tensor([1,1,1,1,1]), torch.tensor([1,1,1,1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
