
Namespace(adaptation_steps=1, data='./data', dataset='mini-imagenet', device=device(type='cuda'), fast_lr=0.001, fine_tune=False, first_order=False, input=32, meta_lr=0.001, model='maml', num_epochs=500000, num_steps=1, num_tasks=32, num_workers=1, shots=1, use_cuda=False, username='hikmatkhan', verbose=False, wand_project='Github_Rapo', wandb_logging=True, ways=5)
Traceback (most recent call last):
  File "/home/hikmat/Desktop/JWorkSpace/Continual-Learning/MetaLearning/MetaLearningHigher.py", line 250, in <module>
    meta_optimizer=meta_optimizer)
  File "/home/hikmat/Desktop/JWorkSpace/Continual-Learning/MetaLearning/MetaLearningHigher.py", line 133, in train_on_task
    qry_loss.backward()
  File "/home/hikmat/anaconda3/envs/PY3/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/hikmat/anaconda3/envs/PY3/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function MulBackward0 returned an invalid gradient at index 0 - expected type torch.FloatTensor but got torch.cuda.FloatTensor