{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "\n",
    "import learn2learn as l2l\n",
    "from learn2learn.data.transforms import FusedNWaysKShots, LoadData, RemapLabels, ConsecutiveLabels\n",
    "\n",
    "from statistics import mean\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)\n",
    "\n",
    "\n",
    "def fast_adapt(batch,\n",
    "               learner,\n",
    "               features,\n",
    "               loss,\n",
    "               adaptation_steps,\n",
    "               shots,\n",
    "               ways,\n",
    "               device=None):\n",
    "\n",
    "    data, labels = batch\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    data = features(data)\n",
    "\n",
    "    # Separate data into adaptation/evaluation sets\n",
    "    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n",
    "    adaptation_indices[np.arange(shots*ways) * 2] = True\n",
    "    evaluation_indices = torch.from_numpy(~adaptation_indices)\n",
    "    adaptation_indices = torch.from_numpy(adaptation_indices)\n",
    "    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n",
    "    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n",
    "\n",
    "    for step in range(adaptation_steps):\n",
    "        train_error = loss(learner(adaptation_data), adaptation_labels)\n",
    "        learner.adapt(train_error)\n",
    "\n",
    "    predictions = learner(evaluation_data)\n",
    "    valid_error = loss(predictions, evaluation_labels)\n",
    "    valid_accuracy = accuracy(predictions, evaluation_labels)\n",
    "    return valid_error, valid_accuracy\n",
    "\n",
    "\n",
    "def perform_experiment(dataset,\n",
    "        ways=5,\n",
    "        shots=5,\n",
    "        meta_lr=0.001,\n",
    "        fast_lr=0.1,\n",
    "        adapt_steps=5,\n",
    "        meta_bsz=32,\n",
    "        iters=1000,\n",
    "        cuda=1,\n",
    "        seed=42,\n",
    "):\n",
    "\n",
    "    cuda = bool(cuda)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cpu')\n",
    "    if cuda and torch.cuda.device_count():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = l2l.vision.datasets.FC100(root='~/data',\n",
    "                                              transform=tv.transforms.ToTensor(),\n",
    "                                              mode='train')\n",
    "    valid_dataset = l2l.vision.datasets.FC100(root='~/data',\n",
    "                                              transform=tv.transforms.ToTensor(),\n",
    "                                              mode='validation')\n",
    "    test_dataset = l2l.vision.datasets.FC100(root='~/data',\n",
    "                                              transform=tv.transforms.ToTensor(),\n",
    "                                             mode='test')\n",
    "    train_dataset = l2l.data.MetaDataset(train_dataset)\n",
    "    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n",
    "    test_dataset = l2l.data.MetaDataset(test_dataset)\n",
    "\n",
    "    train_transforms = [\n",
    "        FusedNWaysKShots(train_dataset, n=ways, k=2*shots),\n",
    "        LoadData(train_dataset),\n",
    "        RemapLabels(train_dataset),\n",
    "        ConsecutiveLabels(train_dataset),\n",
    "    ]\n",
    "    train_tasks = l2l.data.TaskDataset(train_dataset,\n",
    "                                       task_transforms=train_transforms,\n",
    "                                       num_tasks=20000)\n",
    "\n",
    "    valid_transforms = [\n",
    "        FusedNWaysKShots(valid_dataset, n=ways, k=2*shots),\n",
    "        LoadData(valid_dataset),\n",
    "        ConsecutiveLabels(valid_dataset),\n",
    "        RemapLabels(valid_dataset),\n",
    "    ]\n",
    "    valid_tasks = l2l.data.TaskDataset(valid_dataset,\n",
    "                                       task_transforms=valid_transforms,\n",
    "                                       num_tasks=600)\n",
    "\n",
    "    test_transforms = [\n",
    "        FusedNWaysKShots(test_dataset, n=ways, k=2*shots),\n",
    "        LoadData(test_dataset),\n",
    "        RemapLabels(test_dataset),\n",
    "        ConsecutiveLabels(test_dataset),\n",
    "    ]\n",
    "    test_tasks = l2l.data.TaskDataset(test_dataset,\n",
    "                                      task_transforms=test_transforms,\n",
    "                                      num_tasks=600)\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    features = l2l.vision.models.ConvBase(output_size=64, channels=3, max_pool=True)\n",
    "    features = torch.nn.Sequential(features, Lambda(lambda x: x.view(-1, 256)))\n",
    "    features.to(device)\n",
    "    head = torch.nn.Linear(256, ways)\n",
    "    head = l2l.algorithms.MAML(head, lr=fast_lr)\n",
    "    head.to(device)\n",
    "\n",
    "    # Setup optimization\n",
    "    all_parameters = list(features.parameters()) + list(head.parameters())\n",
    "    optimizer = torch.optim.Adam(all_parameters, lr=meta_lr)\n",
    "    loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    for iteration in range(iters):\n",
    "        optimizer.zero_grad()\n",
    "        meta_train_error = 0.0\n",
    "        meta_train_accuracy = 0.0\n",
    "        meta_valid_error = 0.0\n",
    "        meta_valid_accuracy = 0.0\n",
    "        meta_test_error = 0.0\n",
    "        meta_test_accuracy = 0.0\n",
    "        for task in range(meta_bsz):\n",
    "            # Compute meta-training loss\n",
    "            learner = head.clone()\n",
    "            batch = train_tasks.sample()\n",
    "            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                               learner,\n",
    "                                                               features,\n",
    "                                                               loss,\n",
    "                                                               adapt_steps,\n",
    "                                                               shots,\n",
    "                                                               ways,\n",
    "                                                               device)\n",
    "            evaluation_error.backward()\n",
    "            meta_train_error += evaluation_error.item()\n",
    "            meta_train_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "            # Compute meta-validation loss\n",
    "            learner = head.clone()\n",
    "            batch = valid_tasks.sample()\n",
    "            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                               learner,\n",
    "                                                               features,\n",
    "                                                               loss,\n",
    "                                                               adapt_steps,\n",
    "                                                               shots,\n",
    "                                                               ways,\n",
    "                                                               device)\n",
    "            meta_valid_error += evaluation_error.item()\n",
    "            meta_valid_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "            # Compute meta-testing loss\n",
    "            learner = head.clone()\n",
    "            batch = test_tasks.sample()\n",
    "            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                               learner,\n",
    "                                                               features,\n",
    "                                                               loss,\n",
    "                                                               adapt_steps,\n",
    "                                                               shots,\n",
    "                                                               ways,\n",
    "                                                               device)\n",
    "            meta_test_error += evaluation_error.item()\n",
    "            meta_test_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # Print some metrics\n",
    "        print('\\n')\n",
    "        print('Iteration', iteration)\n",
    "        print('Meta Train Error', meta_train_error / meta_bsz)\n",
    "        print('Meta Train Accuracy', meta_train_accuracy / meta_bsz)\n",
    "        print('Meta Valid Error', meta_valid_error / meta_bsz)\n",
    "        print('Meta Valid Accuracy', meta_valid_accuracy / meta_bsz)\n",
    "        print('Meta Test Error', meta_test_error / meta_bsz)\n",
    "        print('Meta Test Accuracy', meta_test_accuracy / meta_bsz)\n",
    "\n",
    "        # Average the accumulated gradients and optimize\n",
    "        for p in all_parameters:\n",
    "            p.grad.data.mul_(1.0 / meta_bsz)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    perform_experiment(dataset='mini-imagenet',\n",
    "    ways=5,\n",
    "    shots=5,\n",
    "    meta_lr=0.001,\n",
    "    fast_lr=0.1,\n",
    "    adapt_steps=5,\n",
    "    meta_bsz=32,\n",
    "    iters=1000,\n",
    "    cuda=1,\n",
    "    seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
